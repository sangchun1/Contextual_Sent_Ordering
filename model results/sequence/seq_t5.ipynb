{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PKO-T5를 이용한 문장 순서 예측\n",
    "\n",
    "이 노트북에서는 paust/pko-t5-base 모델을 사용하여 문장 순서 예측을 구현합니다. 다음과 같은 과정을 수행합니다:\n",
    "\n",
    "1. 데이터 로드 및 전처리\n",
    "2. 모델 및 학습 설정\n",
    "3. Early stopping과 체크포인팅을 적용한 학습\n",
    "4. 모델 평가\n",
    "5. 하이퍼파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "# 결과 저장을 위한 디렉토리 생성\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('history', exist_ok=True)\n",
    "os.makedirs('grid_search_results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "submission_df = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "# 데이터 확인\n",
    "print(\"데이터셋 크기:\", len(train_df))\n",
    "print(\"\\n샘플 데이터:\")\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정제\n",
    "def clean_text(text):\n",
    "  # 특수문자 제거\n",
    "  text = re.sub(r'[^\\w\\s]', '', text)\n",
    "  # 소문자 변환: 한글에는 무의미\n",
    "  text = text.lower()\n",
    "  # 불필요한 공백 제거\n",
    "  text = ' '.join(text.split())\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정제\n",
    "for i in range(4):\n",
    "    train_df[f'sentence_{i}'] = train_df[f'sentence_{i}'].apply(clean_text)\n",
    "    test_df[f'sentence_{i}'] = test_df[f'sentence_{i}'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 설정 및 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceOrderDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=200, device='cuda'):\n",
    "        \"\"\"\n",
    "        문장 순서 예측을 위한 데이터셋 클래스\n",
    "        \n",
    "        Args:\n",
    "            texts: 입력 문장들 ([SEP] 토큰으로 구분됨)\n",
    "            labels: 정답 순서 (공백으로 구분된 숫자 시퀀스)\n",
    "            tokenizer: T5TokenizerFast 또는 AutoTokenizer 인스턴스\n",
    "            max_length: 최대 입력 길이\n",
    "            device: 데이터를 전송할 디바이스\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = str(self.labels[idx])\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        targets = self.tokenizer(\n",
    "            label,\n",
    "            max_length=8,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # 패딩 토큰을 -100으로 설정하여 손실 계산에서 제외\n",
    "        labels = targets['input_ids'].squeeze()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "    def predict_order(self, text):\n",
    "        \"\"\"\n",
    "        문장 순서 예측\n",
    "        \n",
    "        Args:\n",
    "            text: 입력 문장들 ([SEP] 토큰으로 구분됨)\n",
    "        Returns:\n",
    "            predicted_order: 예측된 순서 (정수 리스트)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                max_length=200,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=8,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=4\n",
    "            )\n",
    "            \n",
    "            predicted_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predicted_order = [int(x) for x in predicted_text.split()]\n",
    "            \n",
    "            return predicted_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceOrderPredictor:\n",
    "    def __init__(self, model_name=\"paust/pko-t5-large\", device=None, use_auto_classes=False):\n",
    "        \"\"\"\n",
    "        문장 순서 예측 모델 클래스\n",
    "        \n",
    "        Args:\n",
    "            model_name: 사용할 사전학습 모델 이름\n",
    "            device: 학습에 사용할 디바이스 (GPU/CPU)\n",
    "            use_auto_classes: AutoTokenizer와 AutoModelForSeq2SeqLM 사용 여부\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device if device else ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")  # 디바이스 확인용 출력\n",
    "        \n",
    "        # 모델과 토크나이저 초기화\n",
    "        if use_auto_classes:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "        # 패딩 토큰 설정\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
    "        \n",
    "        # 모델을 명시적으로 지정된 디바이스로 이동\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # 모델이 실제로 지정된 디바이스에 있는지 확인\n",
    "        print(f\"Model device: {next(self.model.parameters()).device}\")\n",
    "    \n",
    "    def prepare_data(self, df, train_size=0.9, random_state=42):\n",
    "        \"\"\"\n",
    "        학습 및 검증 데이터셋 준비\n",
    "        \n",
    "        Args:\n",
    "            df: 입력 데이터프레임\n",
    "            train_size: 학습 데이터 비율\n",
    "            random_state: 랜덤 시드\n",
    "        \"\"\"\n",
    "        texts = df['input_text'].tolist()\n",
    "        labels = df['target_text'].tolist()\n",
    "        \n",
    "        # 데이터 분할\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            texts, labels, train_size=train_size, random_state=random_state\n",
    "        )\n",
    "        \n",
    "        train_dataset = SentenceOrderDataset(\n",
    "            train_texts, \n",
    "            train_labels, \n",
    "            self.tokenizer, \n",
    "            device=self.device  # device 전달\n",
    "        )\n",
    "        val_dataset = SentenceOrderDataset(\n",
    "            val_texts, \n",
    "            val_labels, \n",
    "            self.tokenizer,\n",
    "            device=self.device  # device 전달\n",
    "        )\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def create_dataloaders(self, train_dataset, val_dataset, batch_size=8):\n",
    "        \"\"\"\n",
    "        학습 및 검증용 데이터로더 생성\n",
    "        \n",
    "        Args:\n",
    "            train_dataset: 학습 데이터셋\n",
    "            val_dataset: 검증 데이터셋\n",
    "            batch_size: 배치 크기\n",
    "        \"\"\"\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory=True  # CPU-GPU 전송 최적화\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=8,\n",
    "            pin_memory=True  # CPU-GPU 전송 최적화\n",
    "        )\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def save_checkpoint(self, epoch, model, optimizer, loss, path):\n",
    "        \"\"\"\n",
    "        모델 체크포인트 저장\n",
    "        \n",
    "        Args:\n",
    "            epoch: 현재 에폭\n",
    "            model: 현재 모델 상태\n",
    "            optimizer: 현재 옵티마이저 상태\n",
    "            loss: 현재 손실값\n",
    "            path: 저장 경로\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, path)\n",
    "    \n",
    "    def load_checkpoint(self, path):\n",
    "        \"\"\"\n",
    "        모델 체크포인트 불러오기\n",
    "        \n",
    "        Args:\n",
    "            path: 체크포인트 파일 경로\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return checkpoint\n",
    "    \n",
    "    def save_history(self, history, path):\n",
    "        \"\"\"\n",
    "        학습 히스토리 저장\n",
    "        \n",
    "        Args:\n",
    "            history: 학습 히스토리 딕셔너리\n",
    "            path: 저장 경로\n",
    "        \"\"\"\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(history, f)\n",
    "    \n",
    "    def load_history(self, path):\n",
    "        \"\"\"\n",
    "        학습 히스토리 불러오기\n",
    "        \n",
    "        Args:\n",
    "            path: 히스토리 파일 경로\n",
    "        \"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def predict_order(self, text):\n",
    "        \"\"\"\n",
    "        문장 순서 예측\n",
    "        \n",
    "        Args:\n",
    "            text: 입력 문장들 ([SEP] 토큰으로 구분됨)\n",
    "        Returns:\n",
    "            predicted_order: 예측된 순서 (정수 리스트)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(\n",
    "                text,\n",
    "                max_length=200,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "            \n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=8,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=4\n",
    "            )\n",
    "            \n",
    "            predicted_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            predicted_order = [int(x) for x in predicted_text.split()]\n",
    "            \n",
    "            return predicted_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred_order, true_order):\n",
    "    \"\"\"\n",
    "    예측 순서와 실제 순서 간의 정확도 계산\n",
    "    \n",
    "    Args:\n",
    "        pred_order: 예측된 순서\n",
    "        true_order: 실제 순서\n",
    "    Returns:\n",
    "        정확도 (0 또는 1)\n",
    "    \"\"\"\n",
    "    if isinstance(true_order, str):\n",
    "        true_order = [int(x) for x in true_order.split()]\n",
    "    return int(np.array_equal(pred_order, true_order))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pko-t5-large 모델 초기화\n",
    "predictor = SentenceOrderPredictor()\n",
    "\n",
    "# 데이터셋 준비 (9:1 비율로 분할)\n",
    "train_dataset, val_dataset = predictor.prepare_data(train_df)\n",
    "\n",
    "# 데이터로더 생성\n",
    "batch_size = 4\n",
    "train_loader, val_loader = predictor.create_dataloaders(train_dataset, val_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"학습 데이터 크기: {len(train_dataset)}\")\n",
    "print(f\"검증 데이터 크기: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 학습 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 파라미터\n",
    "num_epochs = 5\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 1000  # Warmup 스텝 수\n",
    "gradient_accumulation_steps = 4  # Gradient accumulation 스텝 수\n",
    "patience = 3  # Early stopping 인내심\n",
    "min_delta = 1e-4  # 최소 개선 기준\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer = AdamW(predictor.model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler 설정\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Mixed precision training을 위한 scaler 설정\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# 학습 히스토리 초기화\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "    'learning_rates': []  # learning rate 기록 추가\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device, tokenizer):\n",
    "    \"\"\"검증 데이터에 대한 모델 평가\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Validation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 예측 생성\n",
    "            predictions = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=8,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=4  # 숫자 중복 방지\n",
    "            )\n",
    "            \n",
    "            # 예측과 레이블을 순서 시퀀스로 변환\n",
    "            pred_texts = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "            label_texts = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
    "            \n",
    "            try:\n",
    "                # 예측과 레이블을 정수 리스트로 변환\n",
    "                pred_orders = [list(map(int, text.split())) for text in pred_texts]\n",
    "                label_orders = [list(map(int, text.split())) for text in label_texts]\n",
    "                \n",
    "                # 정확도 계산\n",
    "                correct = sum(compute_accuracy(pred, label) for pred, label in zip(pred_orders, label_orders))\n",
    "                total_correct += correct\n",
    "                total_samples += len(input_ids)\n",
    "            except ValueError as e:\n",
    "                print(f\"Warning: 잘못된 예측 형식 발견 - {e}\")\n",
    "                print(f\"Predictions: {pred_texts}\")\n",
    "                print(f\"Labels: {label_texts}\")\n",
    "                continue\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)  # 배치 수로 나누기\n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시작 전에 실행\n",
    "# CUDA 캐시 초기화\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 가비지 컬렉션 강제 실행\n",
    "gc.collect()\n",
    "\n",
    "# CUDA 초기화\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.init()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0\n",
    "best_epoch = -1\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 학습 단계\n",
    "    predictor.model.train()\n",
    "    total_train_loss = 0\n",
    "    optimizer.zero_grad()  # 에폭 시작 시 gradient 초기화\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')):\n",
    "        input_ids = batch['input_ids'].to(predictor.device)\n",
    "        attention_mask = batch['attention_mask'].to(predictor.device)\n",
    "        labels = batch['labels'].to(predictor.device)\n",
    "        \n",
    "        # Mixed precision training\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            outputs = predictor.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss / gradient_accumulation_steps  # loss를 gradient accumulation 스텝으로 나눔\n",
    "        \n",
    "        # Scaled backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(predictor.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Optimizer step with scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Learning rate scheduler step\n",
    "            scheduler.step()\n",
    "        \n",
    "        total_train_loss += loss.item() * gradient_accumulation_steps\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # 검증\n",
    "    val_loss, val_accuracy = validate(predictor.model, val_loader, predictor.device, predictor.tokenizer)\n",
    "    \n",
    "    # 히스토리 업데이트\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_accuracy)\n",
    "    history['learning_rates'].append(scheduler.get_last_lr()[0])  # 현재 learning rate 기록\n",
    "    \n",
    "    # 현재 성능 출력\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {avg_train_loss:.4f}')\n",
    "    print(f'Validation Loss: {val_loss:.4f}')\n",
    "    print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "    print(f'Learning Rate: {scheduler.get_last_lr()[0]:.2e}')\n",
    "    \n",
    "    # Early stopping 및 체크포인트 저장 로직\n",
    "    if val_loss < best_val_loss - min_delta:  # 유의미한 개선이 있는 경우\n",
    "        print(f'Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}')\n",
    "        best_val_loss = val_loss\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # 체크포인트 저장\n",
    "        predictor.save_checkpoint(\n",
    "            epoch=epoch,\n",
    "            model=predictor.model,\n",
    "            optimizer=optimizer,\n",
    "            loss=val_loss,\n",
    "            path=f'checkpoints/pko_t5_best_model.pt'\n",
    "        )\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f'No improvement for {patience_counter} epochs (best val_loss: {best_val_loss:.4f} at epoch {best_epoch+1})')\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# 학습 완료 후 최종 결과 출력\n",
    "print('\\n학습 완료!')\n",
    "print(f'Best epoch: {best_epoch+1}')\n",
    "print(f'Best validation loss: {best_val_loss:.4f}')\n",
    "print(f'Best validation accuracy: {best_val_accuracy:.4f}')\n",
    "\n",
    "# 최종 학습 히스토리 저장\n",
    "predictor.save_history(history, 'history/pko_t5_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "predictor.save_history(history, 'history/pko_t5_history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train 및 Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 테스트 데이터 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "predictor = SentenceOrderPredictor()\n",
    "\n",
    "# 체크포인트 로드 시 메모리 효율적인 방식 사용\n",
    "checkpoint = torch.load('./checkpoints/pko_t5_best_model.pt', map_location='cpu')\n",
    "predictor.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "predictor.model.to('cuda')\n",
    "\n",
    "# 불필요한 메모리 해제\n",
    "del checkpoint\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 예측 결과를 저장할 리스트\n",
    "predictions = []\n",
    "\n",
    "# 각 테스트 케이스에 대해 예측 수행\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Predicting\"):\n",
    "    # 입력 텍스트 준비 ([SEP]로 문장들 연결)\n",
    "    input_text = f\"{row['sentence_0']} [SEP] {row['sentence_1']} [SEP] {row['sentence_2']} [SEP] {row['sentence_3']}\"\n",
    "    \n",
    "    # 순서 예측\n",
    "    predicted_order = predictor.predict_order(input_text)\n",
    "    \n",
    "    # 예측 결과를 문자열로 변환 (공백으로 구분)\n",
    "    prediction_str = ' '.join(map(str, predicted_order))\n",
    "    predictions.append(prediction_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과를 제출 형식에 맞게 변환\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'answer_0': [pred[0] for pred in predictions],\n",
    "    'answer_1': [pred[2] for pred in predictions],\n",
    "    'answer_2': [pred[4] for pred in predictions],\n",
    "    'answer_3': [pred[6] for pred in predictions]\n",
    "})\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submission.to_csv('../data/submission_t5_seq.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
